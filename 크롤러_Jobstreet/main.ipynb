{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen, urlretrieve\n",
    "import urllib.request as req\n",
    "import sys\n",
    "import re\n",
    "\n",
    "job=\"trade\"\n",
    "last_page=10\n",
    "\n",
    "a=[]\n",
    "for i in range(1,last_page+1):\n",
    "    page=\"https://www.jobstreet.com.sg/en/job-search/job-vacancy.php?key=\"+job+\"&pg=\"+str(i)+\"\"\n",
    "    a.append(page)\n",
    "\n",
    "\n",
    "page=a[1]\n",
    "page\n",
    "\n",
    "reques= req.Request(page, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "html1=urlopen(reques)\n",
    "soup = BeautifulSoup(html1,\"html.parser\")\n",
    "\n",
    "soup\n",
    "\n",
    "b=[]\n",
    "num=10\n",
    "for j in range(2,num):\n",
    "    url=soup.find_all(\"a\",class_=\"position-title-link\")[j][\"href\"]\n",
    "    url=url.replace(\"§\", \"&\")\n",
    "    b.append(url)\n",
    "\n",
    "b\n",
    "\n",
    "url=soup.find_all(\"a\",class_=\"_3VZAbZG\")[1].find['href']  # url얻어오기 \n",
    "\n",
    "\n",
    "\n",
    "url=b[1]\n",
    "\n",
    "url\n",
    "\n",
    "request= req.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "html=urlopen(request)\n",
    "\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "soup\n",
    "\n",
    "soup2 = soup.find(\"h1\",class_=\"job-position\")   #class로 선택 시\n",
    "soup1 = soup.find(\"img\",{'id':'company_logo'})  #id로 선택 시\n",
    "\n",
    "#img의 경로를 받아온다\n",
    "imgUrl = soup.find(\"img\")[\"data-original\"]\n",
    "\n",
    "res= req.Request(imgUrl, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "save = \"C:/Users/MS/Desktop/logo/downloadImg.png\"\n",
    "\n",
    "image = urlopen(res).read()\n",
    "\n",
    "with open(save, mode=\"wb\") as f: \n",
    "\n",
    "    f.write(image)\n",
    "\n",
    "    print(\"saved\")\n",
    "\n",
    "position = str(soup.find(\"h1\",class_=\"job-position\")) \n",
    "company_name = str(soup.find(\"div\",class_=\"company_name\")) \n",
    "experience = str(soup.find(\"span\",{'id':'years_of_experience'})) \n",
    "location = str(soup.find(\"span\",{'id':'single_work_location'})) \n",
    "description = str(soup.find(\"div\",class_=\"unselectable wrap-text\"))\n",
    "website = str(soup.find(\"a\",{'id':'company_website'}))\n",
    "size = str(soup.find(\"p\",{'id':'company_size'}))\n",
    "industry = str(soup.find(\"p\",{'id':'company_industry'}))\n",
    "overview = str(soup.find(\"div\",{'id':'company_overview_all'}))\n",
    "\n",
    "description=description.replace(\"</li><li>\", \" \")\n",
    "overview=overview.replace(\"</li><li>\", \" \")\n",
    "\n",
    "description=description.replace(\"\\xa0\", \" \")\n",
    "overview=overview.replace(\"\\xa0\", \" \")\n",
    "\n",
    "position = re.sub('<.+?>', '',position, 0).strip()\n",
    "company_name = re.sub('<.+?>', '',company_name, 0).strip()\n",
    "experience = re.sub('<.+?>', '',experience, 0).strip()\n",
    "location = re.sub('<.+?>', '',locate, 0).strip()\n",
    "description = re.sub('<.+?>', '',description, 0).strip()\n",
    "website = re.sub('<.+?>', '',website, 0).strip()\n",
    "size = re.sub('<.+?>', '',size, 0).strip()\n",
    "industry = re.sub('<.+?>', '',industry, 0).strip()\n",
    "overview = re.sub('<.+?>', '',overview, 0).strip()\n",
    "\n",
    "k=100\n",
    "n=9\n",
    "\n",
    "joblist=[[0]*n for k in range(k)]  # n = 열, k = 행\n",
    "\n",
    "z=7\n",
    "\n",
    "url=b[z]\n",
    "request= req.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "html=urlopen(request)\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "position = str(soup.find(\"h1\",class_=\"job-position\")) \n",
    "company_name = str(soup.find(\"div\",class_=\"company_name\")) \n",
    "experience = str(soup.find(\"span\",{'id':'years_of_experience'})) \n",
    "location = str(soup.find(\"span\",{'id':'single_work_location'})) \n",
    "description = str(soup.find(\"div\",class_=\"unselectable wrap-text\"))\n",
    "website = str(soup.find(\"a\",{'id':'company_website'}))\n",
    "size = str(soup.find(\"p\",{'id':'company_size'}))\n",
    "industry = str(soup.find(\"p\",{'id':'company_industry'}))\n",
    "overview = str(soup.find(\"div\",{'id':'company_overview_all'}))\n",
    "\n",
    "description=description.replace(\"</li><li>\", \" \")\n",
    "overview=overview.replace(\"</li><li>\", \" \")\n",
    "\n",
    "description=description.replace(\"\\xa0\", \" \")\n",
    "overview=overview.replace(\"\\xa0\", \" \")\n",
    "\n",
    "position = re.sub('<.+?>', '',position, 0).strip()\n",
    "company_name = re.sub('<.+?>', '',company_name, 0).strip()\n",
    "experience = re.sub('<.+?>', '',experience, 0).strip()\n",
    "location = re.sub('<.+?>', '',locate, 0).strip()\n",
    "description = re.sub('<.+?>', '',description, 0).strip()\n",
    "website = re.sub('<.+?>', '',website, 0).strip()\n",
    "size = re.sub('<.+?>', '',size, 0).strip()\n",
    "industry = re.sub('<.+?>', '',industry, 0).strip()\n",
    "overview = re.sub('<.+?>', '',overview, 0).strip()\n",
    "\n",
    "joblist[z][0]= position\n",
    "joblist[z][1]= company_name\n",
    "joblist[z][2]= experience\n",
    "joblist[z][3]= locate\n",
    "joblist[z][4]= description\n",
    "joblist[z][5]= website\n",
    "joblist[z][6]= size\n",
    "joblist[z][7]= industry\n",
    "joblist[z][8]= overview\n",
    "\n",
    "import pandas as pd\n",
    "joblists=pd.DataFrame(joblist,columns=['position','company_name','experience','location','description','website','size','industry','overview'])\n",
    "joblists.to_csv(\"C:/Users/MS/Desktop/job/job_trade.csv\",index=None)"
   ]
  }
 ]
}